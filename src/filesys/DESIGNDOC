                     +-------------------------+
                     | CS 140                  |
                     | PROJECT 4: FILE SYSTEMS |
                     | DESIGN DOCUMENT         |
                     +-------------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Ruizhongtai Qi <rqi@stanford.edu>
Zhihao Jia <zhihao@stanford.edu>
Tai Guo <taig@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

None.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

None.

		     INDEXED AND EXTENSIBLE FILES
		     ============================

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

-- inode.c --
Change struct inode_disk:
/* On-disk inode.
   Must be exactly BLOCK_SECTOR_SIZE bytes long. */
struct inode_disk
  {
    block_sector_t sector;              /* Sector number of disk location. */
    off_t length;                       /* File size in bytes. */
    unsigned magic;                     /* Magic number. */
    int isdir;                          /* 1 if this inode is dir */
    block_sector_t direct_idx[DIRECT_IDX_CNT];
                                        /* Direct indexes*/
    block_sector_t single_idx;          /* Single indirect indexes*/
    block_sector_t double_idx;          /* Double indirect indexes*/
  };
  
Add struct indirect_block:
/* A sector-size block for indirect indexes*/
struct indirect_block
  {
    block_sector_t idx [SECTOR_IDX_CNT]; /* indexes in an indirect block*/
  };


Add elements into struct inode:
/* In-memory inode. */
struct inode 
  {
    struct list_elem elem;              /* Element in inode list. */
    block_sector_t sector;              /* Sector number of disk location. */
    int open_cnt;                       /* Number of openers. */
    bool removed;                       /* True if deleted, false otherwise. */
    int deny_write_cnt;                 /* 0: writes ok, >0: deny writes. */
    off_t read_length;                  /* File length that can be read now. */
    struct lock inode_lock;             /* lock for inode. */
    struct lock dir_lock;               /* lock for directory. */
    struct inode_disk data;             /* Inode content. */
  };
  
Add static variable:
/* lock for open inode list to avoid race condition. */
static struct lock open_inodes_lock;

Add definitions:
/* Number of indexes in inode's direct block*/
#define DIRECT_IDX_CNT (128 - 6)
/* Number of indexes in a single sector */
#define SECTOR_IDX_CNT (BLOCK_SECTOR_SIZE / 4)
/* Size of direct block in inode_disk */
#define DIRECT_BLOCK_SIZE (DIRECT_IDX_CNT * BLOCK_SECTOR_SIZE)
/* Size of single indirect block in inode_disk */
#define SINGLE_BLOCK_SIZE (SECTOR_IDX_CNT * BLOCK_SECTOR_SIZE)
/* Size of double indirect block in inode_disk */
#define DOUBLE_BLOCK_SIZE (SECTOR_IDX_CNT * SECTOR_IDX_CNT * BLOCK_SECTOR_SIZE)


>> A2: What is the maximum size of a file supported by your inode
>> structure?  Show your work.

The maximum size of a file is 8,516,608 bytes (approximately 8.12MB) in our
design, which can be calculated as follows.

1. The size of direct block: 
S1 = DIRECT_BLOCK_SIZE = 122 * 512 = 62,464 bytes

2. The size of single indirect block:
S2 = SINGLE_BLOCK_SIZE = 128 * 512 = 65,536 bytes

3. The size of double indirect block:
S3 = DOUBLE_BLOCK_SIZE = 128 * 128 * 512 = 8,388,608 bytes

Therefore, the maximum size of a file:
S1 + S2 + S3 = 8,516,608 bytes

---- SYNCHRONIZATION ----

>> A3: Explain how your code avoids a race if two processes attempt to
>> extend a file at the same time.

We provide an API called inode_extend_file, which is the only method to extend
a file. To avoid race condition, we design inode_lock for each inode, which
must be acquired before invocation to inode_extend_file.

If two processes attempt to extend a file at the same time, they have to
compete for inode_lock. The process getting inode_lock has the privilege to
extend the file first, and the other process has to wait for inode_lock until
the first process finishes its execution and release inode_lock.


>> A4: Suppose processes A and B both have file F open, both
>> positioned at end-of-file.  If A reads and B writes F at the same
>> time, A may read all, part, or none of what B writes.  However, A
>> may not read data other than what B writes, e.g. if B writes
>> nonzero data, A is not allowed to see all zeros.  Explain how your
>> code avoids this race.

There are two length variable for each inode, namely inode_disk->length,
and inode->read_length. inode_disk->length is the allocated length of the
inode, while inode->read_length is the readable/initialized length of the
inode. It is always garuanteed that inode->read_length <= inode_disk->length,
and the gap between them is unreadable/uninitialized data.

When process B tries to extend file F, B first acquire inode_lock of file F,
in order to garuantee no other processes can extend the file at the same
time. We implement file extension in a three-phases approach:

Phase 1: Allocate new sectors for the file, and don't write data into new
sectors. At this point, inode_disk->length increases but inode->read_length
remains the same, since the allocated size of file has increased but the
readable size remains.

Phase 2: Write data into new allocated sectors.

Phase 3: Update file's readable length (i.e., inode->read_length) to be equal
to inode_disk->length.

With this three-phases file extension approach, we can avoid race condition.

If A reads before Phase 3, for example A reads during B writes data into
new extended sectors, since we haven't update inode->read_length, A may read
none of what B writes.

If A reads after Phase 3, A may read all/part of what B writes.

A cannot read during Phase 3, since Phase 3 is an atomic operation.

>> A5: Explain how your synchronization design provides "fairness".
>> File access is "fair" if readers cannot indefinitely block writers
>> or vice versa.  That is, many processes reading from a file cannot
>> prevent forever another process from writing the file, and many
>> processes writing to a file cannot prevent another process forever
>> from reading the file.

Readers don't need to acquire locks in our approach, while writers only
need to acquire per file inode_lock if it is going to extend the file.
Therefore, in our system, readers or writers do not block each others from
reading/writing files. Blocking happens only when two writers want to extend
file at the same time.


---- RATIONALE ----

>> A6: Is your inode structure a multilevel index?  If so, why did you
>> choose this particular combination of direct, indirect, and doubly
>> indirect blocks?  If not, why did you choose an alternative inode
>> structure, and what advantages and disadvantages does your
>> structure have, compared to a multilevel index?

Yes, we use a multilevel index design for inode structure.



			    SUBDIRECTORIES
			    ==============

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

Add member:
--inode.c--
struct inode_disk
{
  int isdir;                          /* 1 if this inode is dir */
}

struct inode
{
  struct lock inode_lock;             /* lock for inode. */
}

--thread.h--
struct thread
{
  struct dir *cur_dir;                /* Current working directory */
}


---- ALGORITHMS ----

>> B2: Describe your code for traversing a user-specified path.  How
>> do traversals of absolute and relative paths differ?



---- SYNCHRONIZATION ----

>> B4: How do you prevent races on directory entries?  For example,
>> only one of two simultaneous attempts to remove a single file
>> should succeed, as should only one of two simultaneous attempts to
>> create a file with the same name, and so on.

>> B5: Does your implementation allow a directory to be removed if it
>> is open by a process or if it is in use as a process's current
>> working directory?  If so, what happens to that process's future
>> file system operations?  If not, how do you prevent it?

---- RATIONALE ----

>> B6: Explain why you chose to represent the current directory of a
>> process the way you did.

			     BUFFER CACHE
			     ============

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

-- cache.c --

- New defines:
#define BUFFER_CACHE_SIZE 64           /* Number of cache entries. */
#define CACHE_FLUSH_PERIOD 10          /* Flush period in second. */
#define CACHE_FLUSH_PERIOD_TICKS (TIMER_FREQ * CACHE_FLUSH_PERIOD)

- New structs:
/* Cache entry for the file blocks' cache, i.e. buffer_cache. */
struct cache_entry
{
  block_sector_t sector;               /* Sector number of the cached block. */
  block_sector_t new_sector;           /* Sector no of block after eviction. */
  bool accessed;                       /* Whether has been accessed. */
  bool dirty;                          /* Whether has been written. */
  bool evicting;                       /* Entry being evicted. */
  bool flushing;                       /* Entry being flushed. */
  size_t reader;                       /* Number of active readers. */
  size_t writer;                       /* Number of active writers. */
  size_t waiting_reader;               /* Number of waiting readers. */
  size_t waiting_writer;               /* Number of waiting writers. */
  struct lock lock;                    /* Per entry lock. */
  struct condition ready;              /* Per entry condition variable. */
  uint8_t content[BLOCK_SECTOR_SIZE];  /* Block content. */
};

/* Read-ahead task struct. */
struct read_ahead_task
{
  block_sector_t sector;               /* Sector to read ahead. */
  struct list_elem elem;               /* Element for read_ahead_list. */
};

- New static variables:
/* Buffer cache of file blocks. 
 * In default, as many as 64 blocks can be cached. */
static struct cache_entry buffer_cache[BUFFER_CACHE_SIZE];

/* Global buffer cache lock to prevent two threads from evicting 2 entries
 * for the same sector. Concurrency is kept by releasing the lock before IO. */
static struct lock buffer_cache_lock;

/* Clock hand for eviction algorithm. */
static size_t hand;
static struct lock hand_lock;

/* Read-ahead task queue. */
static struct list read_ahead_list;
static struct lock read_ahead_lock;
static struct condition read_ahead_ready;

---- ALGORITHMS ----

>> C2: Describe how your cache replacement algorithm chooses a cache
>> block to evict.

We used basic clock algorithm for cache eviction (in function evict_entry_id).
A global clock hand variable is maintained. Every time we need to evict a cache
entry, we firstly look at the entry with index number of current clock hand.
Each entry access is protected by a monitor (lock and condition variable).

(1) If the entry is being read/written or some thread is waiting to read/write
it or if the entry is being written back to disk (flushing) or bing evicted,
skip the entry by increasing clock hand by one and look at the next entry.
(2) If the entry is recently accessed (accessed flag set), clear the flag and
skip it to next entry.
(3) At this stage, the entry is known as unaccessed and not being read/written 
and not being waited to access and not in flushing/eviction. It is the entry we
will evict. The entry's evicting flag is set. If the entry's dirty flag is set, 
we flush it to disk before returning its index.

>> C3: Describe your implementation of write-behind.

Our design achieves lazy-writing/write-behind. Cached sector is written back to
disk (I/O) only when (1) it is dirty and (2) the cache entry needs to be 
evicted. Each time before a writing disk I/O happens, the entry's flushing flag
is set, so that other thread can be aware not to read or write the sector in
the meantime. During flushing, all locks are released for best concurrency. 
After the disk I/O finishes, the entry's flushing flag is reset.

A background thread (periodic_flush_daemon) is created at cache initialization.
The daemon periodically flushes buffer cache by calling timer_sleep and 
cache_flush. In default, our design flushes the cache every 10 seconds. 
cache_flush is also called in filesys_done().

>> C4: Describe your implementation of read-ahead.

Our buffer cache design maintains a read-ahead task queue. Each time a read-
ahead task is issued (by calling cache_read_ahead), a read_ahead_task is 
created and pushed back to the queue. Also a signal is sent to the daemon 
thread mentioned below. 

Read ahead task is issued in inode_read_at function when there is more block 
to read in the file. Since it does not do any I/O (which will be handled 
asynchronously by the daemon mentioned below), it returns immediately 
and will not block the current thread.

A background thread (read_ahead_daemon) is created at cache init which
runs a while loop to check whether there is task in the queue: if there is,
it pop the task and execute it by making a cache_read call of the to-be-read-
ahead sector. If the queue is empty, it cond_wait on read_ahead_ready cond.

To prevent wasteful read-ahead (read-ahead task executed after the block is 
read into cache), read-ahead task of sector A is canceled when a process calls
cache_read on sector A before the read-ahead task is executed by the daemon.

---- SYNCHRONIZATION ----

>> C5: When one process is actively reading or writing data in a
>> buffer cache block, how are other processes prevented from evicting
>> that block?

Each cache entry has four counters marking how many threads are waiting to
read or write the block and how many threads are actively reading or writing
the block.

When a thread wants to read or write a block, if firstly goes through the 
cache entries and if it finds one with matching sector number and is not
already being evicted, it increments the entry's waiting_reader or waiting_
writer counter by one. Also, before a thread starts reading or writing data 
to the cached block, reader or writer counter is incremented by one and 
waiting_reader or waiting_writer counter is decremented by one. Only after
reading or writing, is the reader or writer counter decremented back.

On the other side, as described in C2 an entry can be evicted only when all
of its four counters are exactly zero. Since all entry metadata accesses are
synchronized by entry lock, it is guranteed that when a thread is reading,
writing or waiting to read or write a block, the block cannot be evicted.

>> C6: During the eviction of a block from the cache, how are other
>> processes prevented from attempting to access the block?

During an eviction, if the cache entry's previous block A is dirty, A will be 
flushed to disk. Afterwards the new block B will be loaded to the cache entry.
Access to block A is prevented until it has been fully flushed while access to
block B is prevented until block B is fully loaded. More details are in below.

(1) If a cache entry is being evicted, it will not be evicted by another 
process as described in C1.
(2) If thread 1 wants to access block A that is being evicted from an entry by
thread 2, it will wait until block A is flushed by cond_wait on the entry's
condition variable and checks wether the entry is still in flushing state after
wake up, which gurantees that bytes of block A read from disk are updated.
(3) If thread 1 wants to access block B that is going to or being loaded into
an entry by thread 2. It will cond_wait until the entry's evicting flag is 
reset, marking that the block B has been fully loaded.

Note that all cond_wait are waiting without holding locks so that no thread's
I/O would block other threads from running.

---- RATIONALE ----

>> C7: Describe a file workload likely to benefit from buffer caching,
>> and workloads likely to benefit from read-ahead and write-behind.

(1) Buffer caching
File workload with "locality" will benefit from buffer caching. If the process
is actively accessing a certain part of the file, i.e. accessing certain
sectors, these sectors are more likely to be kept in buffer cache so that
most sector access would not require disk IO saving a lot of time.

(2) Read-ahead
If the process accesses a certain range of file and then does some computation
and then accesses the next range of file following the blocks accessed last 
time, then it's beneficial to read ahead the next block from next access. It's
likely that when the process needs to read the next block, it is already in
buffer cache. IO and non-IO operation concurrency is achieved.

(3) Write-behind
One case of file workload is repetitively writing (write multiple 
times) to the same block, which will benefit from write-behind. As long as the block
is not evicted from buffer cache, all block writing will be memory access 
rather than disk IO, saving a lot of time.

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students in future quarters?

>> Any other comments?

