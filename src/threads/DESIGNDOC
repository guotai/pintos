			+--------------------+
			| CS 140             |
			| PROJECT 1: THREADS |
			| DESIGN DOCUMENT    |
			+--------------------+
				   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Tai Guo <taig@stanford.edu>
FirstName LastName <email@domain.example>
FirstName LastName <email@domain.example>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.



---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

In thread.c:
#define NICE_MIN -20                    /* Lowest nice */
#define NICE_MAX 20                     /* Highest nice */
#define NICE_INI 0                      /* Initial nice */
struct thread{
  int nice;                           /* Nice value */
  fixed_point_t recent_cpu;           /* Recent cpu value */
}

In thread.c
fixed_point_t load_avg;

All the nice, recent_cpu and load_avg are needed to calculate the priority 
in the advanced algorithm.


---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  63  61  59     A
 4      4   0   0  62  61  59     A
 8      8   0   0  61  61  59     A
12     12   0   0  60  61  59     B
16     12   4   0  60  60  59     B
20     12   8   0  60  59  59     A
24     16   8   0  59  59  59     A
28     20   8   0  58  59  59     C
32     20   8   4  58  59  58     B
36     20  12   4  58  58  58     B

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

Yes. One ambiguity is whent multiple threads have the highest priority (like 
tick 8, 16, 24, 28 and 36), which thread should be scheduled. In my design, 
the running thread only yields when there are some threads with higher 
priority than it. At tick 8, 16, 24 and 36, the running thread did not 
yield, because it is still one of the threads that have the highest priority.
This design will reduce the need of context switch and is beneficial to the 
performance. And when the running thread yield, if there are multiple 
threads with the highest priority, my design make the choice based on FIFO. 
We have 64 queues and for the scheduler, it just pop the front of the queue 
with the highest priority. For example, C stays in the queue[59] since tick 0 
and B joined the queue[59] later at tick 20, so at tick 28, my design 
choose C to run next.

My scheduler works exactly as the description and the table above. This part 
is a little different from the part 2 regarding of choosing threads when 
multiple threads have the highest priority. This is because we devide the 
project and work roughly independently. Since this is ambiguious in the 
specification and both our policies in part 2 and part 3 have their own 
advantages, I believe it is fine to have both of them in our project.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

Fairly speaking, there is not much choice for us to design regarding this 
question. 

In this prat the only relevant interrupt is timer interrupt. Theoretically 
the work in the timer interrupt should be kept as little as possible, since 
this piece of code will be run every timer tick. If it cost lots of time, 
the threads itself will not have enough time to do its work.

However, some work have to be done in the timer interrupt, like calculating 
load_avg, recent_cpu and priority, because these work must be done at some 
particular time point. And besides that, once every 4 ticks the priorities 
are recalculated, my design also do a small loop to check whether the 
current thread need yield the cpu according to the description in C3.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

Structure:

We use 64 queue in both part 2 and part 3. In this way it is very fast to 
deal with the ready list. The push, pop and delete are all O(1). When all
the priorities are recalculated, moving threads around is also O(n) where 
n is the number of threads, since we just use its priority as the index. 
Searching for the next thread to run costs O(p) where p is the number of 
priority levels (p=64 here) since we need to check all the queues from the 
top priority till the first unempty queue. 

Compared with the single queue implementation, which might need to re-sort 
the queue when recalculating all the priorities which is O(nlogn) but only 
need O(1) to find the next thread to run, our design is better when number 
of threads are large and single queue design is better when number of 
priority levels are much larger than number of threads. I believe for most 
of the OS, threads number is larger than the priority number, so our design 
is a better choice.

Schedule policy:

It is still the case I discussed in C3. A running thread does not yield when 
no thread has higher priority. It must yield as soon as there are other 
threads with higher priority. Another choice is that the running thread 
yield the cpu when it still holds the highest priority but some other 
threads also have the same priority. This choice will result in a context 
switch every 4 ticks. Compared with this choice, my design reduce the number 
of context switches in a resonable way. 

The disadvantage is that I need to check whether it is needed to yield by 
looping from queue[63] to queue[thread_current->priority] to check whether 
empty (see function need_yield()), which cost some time every 4 ticks. 

Actually this problem can be fixed if I have more time. Since in the function
schedule(), it also does the same work as the need_yield() function. We can 
pass in a index from need_yield() to thread_yield() to schedule() to tell it 
the first unempty queue with the highest priority. In this way, the check of 
need of yield brings no extra calculation.

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?



			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?
