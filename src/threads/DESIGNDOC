			+--------------------+
			| CS 140             |
			| PROJECT 1: THREADS |
			| DESIGN DOCUMENT    |
			+--------------------+
				   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Tai Guo <taig@stanford.edu>
Ruizhongtai Qi <rqi@stanford.edu>
Zhihao Jia <zhihao@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

None.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

None that we can think of.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

-- thread.h --
New memebers in struct thread:
  /* List element for the sleeping list */
  struct list_elem sleepelem;
    
  /* The time this thread will wake up */
  int64_t wake_up_time;

-- timer.c --
New struct in 'timer.c'
  /* List of all threads that are blocked in timer_sleep*/
  static struct list sleep_list;

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

In function timer_sleep(), we first write down the current time ticks,
and use a 64-bit variable 'start' to save current time tick number.
By adding 'start' with the number of time ticks we have to wait, we can
calucluate the wake up time, by which time this thread should return to
ready list and be scheduled to run. During this time slot, we put current
thread into a sleep list, which is implemented to keep track of all sleeping
threads. Since this list is a global view of sleeping threads and can be
modified by any thread, we disable interrupts during our operations on
sleep list. After that, current thread invokes thread_block(), which changes
the status of current thread to BLOCKED and schedule other threads to run.
One thing to consider is that schedule() requires interrupts to be disabled,
since it involves context switch, and this is the second reason why we disable
interrupts in timer_sleep().

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

We invovle sorted list to implement sleep list, which achieves O(1) time
complexity to find the thread with earliest wake-up time. This can optimize
timer interrupt handler, since it only takes O(1) time to find the next thread
to wake up.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

In order to avoid race conditions, we disable interrupts during our operations
on sleep list and blocking current thread. When multiple threads call
timer_sleep() simultaneously, only one thread can manipulate sleep list and 
block itself at any time. Operation on sleep list can be considered atomic.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

In order to avoid race conditions when a timer interrupt occurs during a call
to timer_sleep(), we disable interrupts when either thread is manipulating the
sleep list, which is shared between timer_sleep() and timer interrupt handler.
As a result, when timer interrupt occurs in timer_sleep(), the execution of
timer interrupt handler will be deferred until timer_sleep() finishes
manipulation on sleep list and blocks itself. So race conditions are avoided.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

Our design has several advantages.

First, we use sleep list to keep track of all threads that is currently
sleeping. Instread of spinning in a loop and calling thread_yield() until
enough time has gone by, timer_sleep() add current thread into sleep list
and remove current thread from thread ready list by invoking thread_block()
and therefore avoids 'busy wait'.

Second, we use sorted list to implement sleep list, which achieves O(1) time
complexity to find the thread with earliest wake-up time. This can optimize
timer interrupt handler, since it only takes O(1) time to find the next thread
to wake up.

Third, list elements of sleep list is located in thread structure, which avoids
dynamic allocation every time a thread is added into sleep list. In the thread
structure, we added a list_elem (i.e., sleepelem) and a 64 bit integer 
(i.e., wake_up_time). The list_elem is to keep tack of its previous and next
element in sleep list, wile the 64 bit integer records its wake-up time. By
only introducing 12 bytes overhead for each thread, we entirely avoid dynamic
allocation during the manipulation on sleep list. 

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

-- thread.h --
New members in struct thread:
  int eff_priority;                 /* Effective priority. */
  struct list acquired_locks_list;  /* Locks acquired by this thread. */
                                       Used for priority donation. */
  struct lock *lock_to_acquire;     /* The lock this thread is waiting on. */
                                       Used for priority chaining. */
-- synch.h --
New member in struct lock:
  struct list_elem lockelem;        /* List element for acquired_locks_list. */

-- thread.c --
Changes to static vairable ready_list:
  static struct list ready_list[PRI_MAX+1]; /* One list for each priority. */


>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

We use three extra data structures to realize priority donation. Priority 
donations (i.e. information on which threads current thread donates to and 
which threads donate to current threads) are not explicitly checked. 
Instead we records which lock a thread is waiting on (lock_to_acquire) and
what locks a thread hold (acquired_locks_list) and use them to infer the
priority donations.

(1) int eff_priority in struct thread
The struct member eff_priority is effective priority of a thread which is the 
maximum of the thread's generic priority and eff_priorities of the threads 
waiting on it's acquired locks (aka. donated priorities). All priority 
scheduling decisions are made by looking at eff_priorities.

(2) struct list acquired_locks_list in struct thread
The acquired_locks_list is used when we need to update a thread's eff_priority,
which happens during the donation "give-back" after releasing a held lock.

(3) struct lock *lock_to_acquire in struct thread
The lock_to_acquire of a thread is the lock the thread is waiting on. It is 
added for donation chaining / nested donation, which will be more explained
in B4 in more detail.

There are two cases that need careful considerations for priority donation 
tracking: nested donation and multiple-source donations.

(a) Nested donation
Nested donation is when a donation in lock_acquire triggers a chaining 
effect of priority donation. We used lock_to_acquire to determine whether
and to which thread we will do nested donatation.

When current thread (Thread 1 in diagram below) acquires a lock (Lock A) and 
blocks, if it's eff_priority is greater than the lock holder (Thread 2)'s 
eff_priority, Thread 1 will "donate" priority to the Thread 2 by calling a 
function to set Thread 2's eff_priority, which considers nested donation.
Inside the set function, we check if this Thread 2 is also waiting on a lock
by looking at if it's lock_to_acquire is set. If it is, like the case in the 
diagram, Thread 2 will "donate" priority to the lock's holder (Thread 3) by 
recursively calling the set eff_priority function. 

  * Thread 1 *         * Thread 2 *          * Thread 3 *
            \             /      \             /       \         
          waiting on  held by  waiting on  held by   waiting on
              \         /          \         /           \
              [ Lock A ]           [ Lock B ]            ...

(b) Multiple-source donations
Multiple-source donations happens when multiple threads are waiting on the 
same lock. The effective priority will be the highest eff_priority of the 
donators/lock waiters. One thing need attention is to update a multi-donated
thread's eff_priority after it releases a lock. acquired_locks_list, together 
with lock's sema.waiters are used for eff_priority update.

To illustrate the process of updating a multi-donated thread's eff_priority
in releasing a lock, we will look at the diagram as an example. In the diagram
below, three threads T1, T2, T3 are waiting on Lock A held by T0 and T4 is 
waiting on Lock B held also by T0.
Due to donation rule T0 has high priority since High T1 is waiting on its lock. 
However, when T0 releases Lock A, High T1 will get the lock and the donation 
is no longer valid. In this case, T0 needs to sweep locks in it's 
acquired_locks_list and go through each thread waiting on those locks to see 
what the next highest donated priority is. In this diagram, it's Medium T4 
waiting on T0's acquired Lock B. 

  * High T1 *         * Low T0 *
        \             /         \
      waiting on  held by      held by
          \         /             \
          [ Lock A ]              [ Lock B ]
          /         \                      \
      waiting on  waiting on             waiting on
        /             \                      \
  * Low T2 *         * Low T3 *          * Medium T4 *

 
---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

During a lock release (in sema_up), sema_up or cond_signal (in sema_up) we 
will extract the highest eff_priority thread from waiting list and yield 
current thread if it's larger than current eff_priority. 

We used list_min function and thread_priority_greater comparator to get 
the highest eff_priority thread from the waiting list.
The reason we are not using list_push_back and list_inset_ordered is that
the thread's eff_priority may change over time and the order at the time
of insertion may be outdated. Use list_min inside a critical zone (interrupt
diabled) gurantees we will get the up-to-date highest eff_priority thread.

Also, in order to garantee the ready_list is up-to-date, we update the 
ready_list everytime we reset a thread's eff_priority so that the highest
eff_priority thread is guranteed to run first after a thread calls 
thread_yield.


>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

When current thread acquires a lock and blocks, if it's eff_priority is 
greater than the lock holder's eff_priority, current thread will "donate" 
priority to the lock holder by calling a function to set its eff_priority, 
which also considers nested donation. Inside the set function, we check 
if this thread (the lock holder) is also waiting on a lock.
If it is and it's priority is higher than that of the lock's holder, 
it will nestly "donate" by recursively calling the set eff_priority function.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

When current thread releases a lock, it extracts the highest priority thread 
(t1) from the lock's waiters list. If t1's eff_priority equals to the
current's, then it's possible that current's eff_priority is the result of 
donation from t1 and may need to "give back" the donation in releasing 
the lock. In this case, we will call thread_update_eff_priority to update 
current thread's eff_priority. What this function does is checking all waiters 
on locks acquired by current thread and see what is the highest eff_priority 
of these waiters and set current thread's eff_priority to it. It's possible
for a nested donation to happen in this case as well.


---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

thread_set_priority() not only updates its generic priority, but also
recalculates its effective priority. A thread's effective priority is the 
maximum value of its generic priority and effective priority of the threads 
waiting on its acquired locks. 

A potential race occurs in thread_set_priority() if interrupt comes in right
after thread#1 has computed its effective priority before not yet assigned it
to  thread->eff_priority. During the interruption period, if a new thread#2 
with a higher priority comes in and waits on a lock acquired by thread#1. Then,
we should promote thread#1's eff_priority to that of thread#2. When thread#1
switchs back, we will assign its old effective priority to
thread#1->eff_priority, without considering the affects of thread#2.

A lock cannot solve this race, since lock cannot avoid interrupts, and
consequently let thread#1 ignore the affects of thread#2. Moreover, computing
this new lock may lead new priority donation which futher changes efftive
priority. To avoid this race, we disable interrupt inside 
thread_set_priority().

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

To solve the nested donation problem, we implement a recursive function
thread_set_eff_priority() to find all threads, whose effective priority need
changes when a lock_acquire(), lock_release(), or thread_set_priority()
happens.

One alternative approach is to give each thread a donating list, which keeps
track of all other threads that current thread needs to donate its effective 
priority to. This approach involves several downsides, compared with our
approach.

First, it introduce a high time complexity (O(n^2), where n is the
total number of threads in OS) for lock_acquire(), lock_release(), and 
thread_set_priority(). The worst case is that current thread has to donate its
priority to all other threads, in which case you have to check donating lists 
of all other threads, and achieves O(n^2) time complexity since you may have to
check O(n) lists and each list may have O(n) threads. In our design, we only
keep track of the locks current thread has acquired. For lock_acquire(),
lock_release(), and thread_set_priority(), we only introduce O(n) time 
complexity since we go over other thread at most once.

Second, it involves a high space overhead for each thread. In this design,
donating list keeps track of all other threads that current thread has to
donate its effective priority to. In worst case, it may consume O(n) memory,
since it may has to donating to all other threads. In the contrary, our
approach only keeps track of the locks current thread has acquired. And since
each lock can be acquired by at most one thread. We only have to introduce a
struct list (which is actually two pointers) to each thread. Therefore, we
add O(1) overhead to each thread.

Third, it involves dynamic allocation, since we have to allocate new memory
every time we add a new thread into donating list. Our design eliminate the 
cost of dynamic allocation by tracking the set of locks current thread has
acquired. Since each lock shows up in almost one thread's acquired_lock_list,
we don't have to allocate memory when a lock is added into acquired_lock_list.

			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

-- thread.h --
New defines:
#define NICE_MIN -20                    /* Lowest nice */
#define NICE_MAX 20                     /* Highest nice */
#define NICE_INI 0                      /* Initial nice */

New members in struct thread:
struct thread{
  int nice;                           /* Nice value */
  fixed_point_t recent_cpu;           /* Recent cpu value */
}

-- thread.c --
New global variable:
fixed_point_t load_avg;

All the nice, recent_cpu and load_avg are needed to calculate the priority 
in the advanced algorithm.


---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  63  61  59     A
 4      4   0   0  62  61  59     A
 8      8   0   0  61  61  59     A
12     12   0   0  60  61  59     B
16     12   4   0  60  60  59     B
20     12   8   0  60  59  59     A
24     16   8   0  59  59  59     A
28     20   8   0  58  59  59     C
32     20   8   4  58  59  58     B
36     20  12   4  58  58  58     B

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

Yes. One ambiguity is whent multiple threads have the highest priority (like 
tick 8, 16, 24, 28 and 36), which thread should be scheduled. In my design, 
the running thread only yields when there are some threads with higher 
priority than it. At tick 8, 16, 24 and 36, the running thread did not 
yield, because it is still one of the threads that have the highest priority.
This design will reduce the need of context switch and is beneficial to the 
performance. And when the running thread yield, if there are multiple 
threads with the highest priority, my design make the choice based on FIFO. 
We have 64 queues and for the scheduler, it just pop the front of the queue 
with the highest priority. For example, C stays in the queue[59] since tick 0 
and B joined the queue[59] later at tick 20, so at tick 28, my design 
choose C to run next.

My scheduler works exactly as the description and the table above. This part 
is a little different from the part 2 regarding of choosing threads when 
multiple threads have the highest priority. This is because we devide the 
project and work roughly independently. Since this is ambiguious in the 
specification and both our policies in part 2 and part 3 have their own 
advantages, I believe it is fine to have both of them in our project.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

Fairly speaking, there is not much choice for us to design regarding this 
question. 

In this prat the only relevant interrupt is timer interrupt. Theoretically 
the work in the timer interrupt should be kept as little as possible, since 
this piece of code will be run every timer tick. If it cost lots of time, 
the threads itself will not have enough time to do its work.

However, some work have to be done in the timer interrupt, like calculating 
load_avg, recent_cpu and priority, because these work must be done at some 
particular time point. And besides that, once every 4 ticks the priorities 
are recalculated, my design also do a small loop to check whether the 
current thread need yield the cpu according to the description in C3.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

Structure:

We use 64 queue in both part 2 and part 3. In this way it is very fast to 
deal with the ready list. The push, pop and delete are all O(1). When all
the priorities are recalculated, moving threads around is also O(n) where 
n is the number of threads, since we just use its priority as the index. 
Searching for the next thread to run costs O(p) where p is the number of 
priority levels (p=64 here) since we need to check all the queues from the 
top priority till the first unempty queue. 

Compared with the single queue implementation, which might need to re-sort 
the queue when recalculating all the priorities which is O(nlogn) but only 
need O(1) to find the next thread to run, our design is better when number 
of threads are large and single queue design is better when number of 
priority levels are much larger than number of threads. I believe for most 
of the OS, threads number is larger than the priority number, so our design 
is a better choice.

Schedule policy:

It is still the case I discussed in C3. A running thread does not yield when 
no thread has higher priority. It must yield as soon as there are other 
threads with higher priority. Another choice is that the running thread 
yield the cpu when it still holds the highest priority but some other 
threads also have the same priority. This choice will result in a context 
switch every 4 ticks. Compared with this choice, my design reduce the number 
of context switches in a resonable way. 

The disadvantage is that I need to check whether it is needed to yield by 
looping from queue[63] to queue[thread_current->priority] to check whether 
empty (see function need_yield()), which cost some time every 4 ticks. 

Actually this problem can be fixed if I have more time. Since in the function
schedule(), it also does the same work as the need_yield() function. We can 
pass in a index from need_yield() to thread_yield() to schedule() to tell it 
the first unempty queue with the highest priority. In this way, the check of 
need of yield brings no extra calculation.

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?



			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?
